{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rm8XL_PpeXIm"
      },
      "outputs": [],
      "source": [
        "#!pip install pyLDAvis==2.1.2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hsse6XBqeVD7"
      },
      "outputs": [],
      "source": [
        "from gensim.models import LdaModel\n",
        "import gensim\n",
        "from gensim.models.coherencemodel import CoherenceModel\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from gensim.test.utils import datapath\n",
        "import spacy\n",
        "import gc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PgHL5cktTNpK"
      },
      "outputs": [],
      "source": [
        "#Ruben's stuff\n",
        "from google.cloud import bigquery\n",
        "from google.oauth2 import service_account\n",
        "import json, os\n",
        "\n",
        "# Google Cloud services\n",
        "gcp_service_account_credentials_json_filename = 'epfl-course-f41b0ed796f9.json' #need to upload the json credential files to the root directory of the google colab files\n",
        "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = gcp_service_account_credentials_json_filename\n",
        "credentials = service_account.Credentials.from_service_account_file(gcp_service_account_credentials_json_filename, scopes=['https://www.googleapis.com/auth/bigquery', 'https://www.googleapis.com/auth/drive'])\n",
        "project_id = 'epfl-course'\n",
        "bigquery_client = bigquery.Client(credentials=credentials, project=project_id)\n",
        "bigquery_client = bigquery.Client()\n",
        "\n",
        "def bq_execute_query(query, mode=\"INTERACTIVE\", wait=False, to_dataframe=False):\n",
        "    job_config = bigquery.QueryJobConfig(priority=\"bigquery.QueryPriority.{}\".format(mode)) # Run at BATCH priority, which won't count toward concurrent rate limit, otherwise INTERACTIVE.\n",
        "    query_job = bigquery_client.query(query, job_config)\n",
        "    if wait==True:\n",
        "        print(\"Executed BQ query: \", query_job.result())\n",
        "    if to_dataframe==True:\n",
        "        return(query_job.to_dataframe())\n",
        "    else:\n",
        "        return(query_job)\n",
        "\n",
        "def upload_df_to_bq(df, bq_destination_table, write_disposition=\"WRITE_APPEND\"):\n",
        "    #bq_table_name = \"epfl-course.dataset.table\"\n",
        "    job_config = bigquery.LoadJobConfig(create_disposition=\"CREATE_IF_NEEDED\", write_disposition=write_disposition) #write_disposition=\"WRITE_TRUNCATE\" in order to delete all the data from old table and insert new data\n",
        "    upload_df_to_bq_job = bigquery_client.load_table_from_dataframe(\n",
        "        df, bq_destination_table, job_config = job_config)\n",
        "    print(\"Uploaded DF to BQ: \",upload_df_to_bq_job.result()) \n",
        "\n",
        "def upload_json_to_bq(json_object, bq_table):\n",
        "    try:\n",
        "        job_config = bigquery.LoadJobConfig()\n",
        "        job_config.autodetect = False #Change to True if the table on BQ does not exits\n",
        "        job_config.max_bad_records = 0\n",
        "        job_config.ignore_unknown_values = True\n",
        "        job_config.source_format = 'NEWLINE_DELIMITED_JSON'\n",
        "        job_config.create_disposition= \"CREATE_IF_NEEDED\"\n",
        "        job_config.write_disposition= \"WRITE_APPEND\"\n",
        "        job_config.schema_to_json(schema_table)\n",
        "        job = bigquery_client.load_table_from_file(json_object, bq_table, job_config = job_config)\n",
        "        print(\"Loaded JSON to BQ table {} as job {}\".format(bq_table, job.result()))\n",
        "        assert job.job_type == 'load'\n",
        "        assert job.state == 'DONE'\n",
        "    except:\n",
        "        print(\"ERROR Could not load JSON to BQ table {} as job {}\".format(bq_table, job.result()))\n",
        "\n",
        "def upload_file_to_gcs(filename, new_filename, folder=''):\n",
        "    folder = folder if folder == '' else folder + '/'\n",
        "    bucket = storage_client.get_bucket(CLOUD_STORAGE_BUCKET)\n",
        "    blob = bucket.blob('{folder}{file}'.format(folder=folder,\n",
        "                                               file=new_filename))\n",
        "    blob.upload_from_filename(filename)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tH3wPLEfjl9k"
      },
      "outputs": [],
      "source": [
        "#We train LDA on two months of quotes\n",
        "sexy_query = \"\"\"\n",
        "SELECT \n",
        "  quoteId,\n",
        "  lemmas,\n",
        "  stems,\n",
        "  geoNames,\n",
        "FROM\n",
        "  `epfl-course.ada_project.ste`\n",
        "WHERE\n",
        "  DATE(LEFT(quoteid, 10)) between \"2016-01-01\" and \"2016-02-28\"\n",
        "\"\"\"\n",
        "df = bq_execute_query(sexy_query, to_dataframe=True)\n",
        "print('Dates from', df.quoteId.min()[:10])\n",
        "print('Dates from', df.quoteId.max()[:10])\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TbXyh-bJejq4"
      },
      "outputs": [],
      "source": [
        "#import locally saved version of quotes, filtered by language\n",
        "good_quotes = pd.read_csv('filtered_quotes.csv.gz')\n",
        "good_quotes.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eRW2onqWe0k7"
      },
      "outputs": [],
      "source": [
        "print('Good quotes', len(good_quotes))\n",
        "print('all quotes', len(df))\n",
        "print('Discarded quotes', len(df) - len(good_quotes))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YFuhhtwXfDe5"
      },
      "outputs": [],
      "source": [
        "#select only the quotes in english\n",
        "df.set_index('quoteId', inplace = True)\n",
        "good_quotes.set_index('quoteId', inplace = True)\n",
        "df = df.loc[good_quotes.index]\n",
        "#create the dictionary\n",
        "dictionary = gensim.corpora.Dictionary(df.stems.values)\n",
        "#use the dictionary to get the bow\n",
        "corpus = [dictionary.doc2bow(doc) for doc in df.stems.values]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V-l7t5EGDoni"
      },
      "outputs": [],
      "source": [
        "#tain model \n",
        "lda_model =  gensim.models.LdaMulticore(corpus, \n",
        "                                  num_topics = 9, \n",
        "                                  id2word = dictionary,                                    \n",
        "                                  passes = 10,\n",
        "                                  workers = 5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#save model\n",
        "coherence_model_lda = CoherenceModel(model=lda_model, texts=df.stems.values, dictionary=dictionary, coherence='c_v')\n",
        "coherence_lda = coherence_model_lda.get_coherence()\n",
        "print(\"number of topics \", 9,\"coherence_value :\" , coherence_lda)\n",
        "\n",
        "# Save model to disk.\n",
        "temp_file = datapath(\"./1_layer_model\")\n",
        "lda_model.save(\"1_layer_model.txt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LQFAdDfCurQv"
      },
      "outputs": [],
      "source": [
        "#train model\n",
        "lda_model =  gensim.models.LdaMulticore(corpus, \n",
        "                                  num_topics = 30, \n",
        "                                  id2word = dictionary,                                    \n",
        "                                  passes = 10,\n",
        "                                  workers = 5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w1iVc5RFTPy8"
      },
      "outputs": [],
      "source": [
        "#save model\n",
        "coherence_model_lda = CoherenceModel(model=lda_model, texts=df.stems.values, dictionary=dictionary, coherence='c_v')\n",
        "coherence_lda = coherence_model_lda.get_coherence()\n",
        "print(\"number of topics \",30,\"coherence_value :\" , coherence_lda)\n",
        "\n",
        "# Save model to disk.\n",
        "temp_file = datapath(\"./1_layer_model30\")\n",
        "lda_model.save(\"1_layer_model30.txt\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "08TCK-i0zBBd"
      },
      "source": [
        "# Evaluating the model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SDfGT8kT3x_E"
      },
      "outputs": [],
      "source": [
        "#evaluating on quotes from march \n",
        "sexy_query = \"\"\"\n",
        "SELECT \n",
        "  quoteId,\n",
        "  lemmas,\n",
        "  stems,\n",
        "  geoNames,\n",
        "FROM\n",
        "  `epfl-course.ada_project.quote_preprocessed_spacy_with_geo`\n",
        "WHERE\n",
        "  DATE(LEFT(quoteid, 10)) between \"2016-03-01\" and \"2016-03-31\"\n",
        "\"\"\"\n",
        "test_df = bq_execute_query(sexy_query, to_dataframe=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vir6oD0_4jTW"
      },
      "outputs": [],
      "source": [
        "print(len(test_df))\n",
        "test_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AeosI2lN1TiR"
      },
      "outputs": [],
      "source": [
        "#30 topics\n",
        "from pprint import pprint\n",
        "lda_model_30 = LdaModel.load('1_layer_model30.txt')\n",
        "dictionary_30 = gensim.corpora.Dictionary.load('1_layer_model30.txt.id2word')\n",
        "\n",
        "coherence_model_lda = CoherenceModel(model=lda_model_30, texts=test_df.stems.values, dictionary=dictionary_30, coherence='c_v')\n",
        "coherence_lda = coherence_model_lda.get_coherence()\n",
        "print(\"number of topics \",30,\"coherence_value :\" , coherence_lda)\n",
        "\n",
        "# Print the Keyword in the 10 topics\n",
        "pprint(lda_model_30_topics.print_topics())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NngKTZhx7T_3"
      },
      "outputs": [],
      "source": [
        "!pip install pyLDAvis==2.1.2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SxGsBqx27K4C"
      },
      "outputs": [],
      "source": [
        "import pyLDAvis.gensim\n",
        "import pickle \n",
        "import pyLDAvis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EbfDGnjszC_l"
      },
      "outputs": [],
      "source": [
        "#10 topics\n",
        "from pprint import pprint\n",
        "lda_model_10 = LdaModel.load('1_layer_model.txt')\n",
        "dictionary_10 = gensim.corpora.Dictionary.load('1_layer_model.txt.id2word')\n",
        "\n",
        "coherence_model_lda = CoherenceModel(model=lda_model_10, texts=test_df.stems.values, dictionary=dictionary_10, coherence='c_v')\n",
        "coherence_lda = coherence_model_lda.get_coherence()\n",
        "print(\"number of topics \",9,\"coherence_value :\" , coherence_lda)\n",
        "\n",
        "# Print the Keyword in the 10 topics\n",
        "#pprint(lda_model_10_topics.print_topics())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gDaOwq8s7ctN"
      },
      "outputs": [],
      "source": [
        "# Visualize the topics\n",
        "pyLDAvis.enable_notebook()\n",
        "corpus = [dictionary_10.doc2bow(doc) for doc in test_df.stems.values]\n",
        "LDAvis_prepared = pyLDAvis.gensim.prepare(lda_model_10, corpus, dictionary_10)\n",
        "LDAvis_prepared"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BJIGJ1Cz8upq"
      },
      "outputs": [],
      "source": [
        "pyLDAvis.enable_notebook()\n",
        "corpus = [dictionary_30.doc2bow(doc) for doc in test_df.stems.values]\n",
        "LDAvis_prepared = pyLDAvis.gensim.prepare(lda_model_30, corpus, dictionary_30)\n",
        "LDAvis_prepared"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LNNiHqfVAVwo"
      },
      "source": [
        "# Select the best model\n",
        "\n",
        "Aggregate the topics about politics in Lda_9 "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HlEZGLd8BDc1"
      },
      "outputs": [],
      "source": [
        "sub_corpus = [dictionary_10.doc2bow(doc) for doc in sub_df.stems.values]\n",
        "predictions = lda_model_10[sub_corpus]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JjhWSaqYHLgB"
      },
      "outputs": [],
      "source": [
        "scores = []\n",
        "for p in predictions:\n",
        "  tot_score = 0\n",
        "  for s in p:\n",
        "    tot_score += s[1]\n",
        "  scores.append(tot_score)\n",
        "scores = np.array(scores)\n",
        "scores.min()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FGfYJA2JBijt"
      },
      "outputs": [],
      "source": [
        "#reformat the prediction of the model to get the main topic for each quote\n",
        "politics = [8, 5, 4, 0, 3]\n",
        "sport = [7]\n",
        "misc = [1, 2]\n",
        "art = [6]\n",
        "#reformat topics:\n",
        "prediction_reformatted = []\n",
        "for doc in predictions:\n",
        "  politics_score = 0\n",
        "  misc_score = 0\n",
        "  sport_score = 0\n",
        "  art_score = 0\n",
        "  for t in doc:\n",
        "    #check if politics\n",
        "    if t[0] in politics:\n",
        "      politics_score += t[1]\n",
        "      continue\n",
        "\n",
        "    #check if misc\n",
        "    if t[0] in misc:\n",
        "      misc_score += t[1]\n",
        "      continue\n",
        "    \n",
        "    #check if sport\n",
        "    if t[0] in sport:\n",
        "      sport_score += t[1]\n",
        "      continue\n",
        "\n",
        "    #check if art\n",
        "    if t[0] in art:\n",
        "      art_score += t[1]\n",
        "      continue\n",
        "\n",
        "  erf = {\n",
        "      'politics&biz': politics_score,\n",
        "      'sport': sport_score,\n",
        "      'art': art_score,\n",
        "      'misc':misc_score,\n",
        "  }\n",
        "  main = dict()\n",
        "  main['main'] = max(erf, key=erf.get)\n",
        "  main['score'] = erf[main['main']]\n",
        "  erf = {\n",
        "      'scores': erf,\n",
        "      'main': main,\n",
        "      'tot': politics_score + sport_score + art_score + misc_score\n",
        "  }\n",
        "  prediction_reformatted += [erf]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b2bCFyLUJwAG"
      },
      "outputs": [],
      "source": [
        "norms = [p['tot'] for p in prediction_reformatted]\n",
        "norms = np.array(norms)\n",
        "(abs(norms - scores) < 10**5).all()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lN8x7g-fMVXA"
      },
      "outputs": [],
      "source": [
        "confidence = [p['main']['score'] for p in prediction_reformatted]\n",
        "topic = [p['main']['main'] for p in prediction_reformatted]\n",
        "sub_df['score'] = confidence\n",
        "sub_df['topic'] = topic"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yBGCiQI2M8WW"
      },
      "outputs": [],
      "source": [
        "sub_df[['stems','score', 'topic']].head(20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uNNhSbllMdrk"
      },
      "outputs": [],
      "source": [
        "sub_df[sub_df['score']> .6].groupby(['topic']).mean()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O3949QhuMjps"
      },
      "outputs": [],
      "source": [
        "#divide the dataset\n",
        "corpus = [dictionary_10.doc2bow(doc) for doc in df.stems.values]\n",
        "print(len(corpus))\n",
        "predictions = lda_model_10[corpus]\n",
        "\n",
        "#transform the predictions and aggregate the similar topics \n",
        "politics = [8, 5, 4, 0, 3]\n",
        "sport = [7]\n",
        "misc = [1, 2]\n",
        "art = [6]\n",
        "#reformat topics:\n",
        "prediction_reformatted = []\n",
        "for doc in predictions:\n",
        "  politics_score = 0\n",
        "  misc_score = 0\n",
        "  sport_score = 0\n",
        "  art_score = 0\n",
        "  for t in doc:\n",
        "    #check if politics\n",
        "    if t[0] in politics:\n",
        "      politics_score += t[1]\n",
        "      continue\n",
        "\n",
        "    #check if misc\n",
        "    if t[0] in misc:\n",
        "      misc_score += t[1]\n",
        "      continue\n",
        "    \n",
        "    #check if sport\n",
        "    if t[0] in sport:\n",
        "      sport_score += t[1]\n",
        "      continue\n",
        "\n",
        "    #check if art\n",
        "    if t[0] in art:\n",
        "      art_score += t[1]\n",
        "      continue\n",
        "\n",
        "  erf = {\n",
        "      'politics&biz&tech': politics_score,\n",
        "      'sport': sport_score,\n",
        "      'art': art_score,\n",
        "      'misc':misc_score,\n",
        "  }\n",
        "  main = dict()\n",
        "  main['main'] = max(erf, key=erf.get)\n",
        "  main['score'] = erf[main['main']]\n",
        "  erf = {\n",
        "      'scores': erf,\n",
        "      'main': main,\n",
        "      'tot': politics_score + sport_score + art_score + misc_score\n",
        "  }\n",
        "  prediction_reformatted += [erf]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ktde7CFeZby9"
      },
      "outputs": [],
      "source": [
        "#creating df out of the reformatted detected topics\n",
        "confidence = [p['main']['score'] for p in prediction_reformatted]\n",
        "topic = [p['main']['main'] for p in prediction_reformatted]\n",
        "spectrum = [p['scores'] for p in prediction_reformatted]\n",
        "df['score'] = confidence\n",
        "df['topic'] = topic\n",
        "df['spectrum'] = spectrum\n",
        "df.head(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XwyIANXzauTd"
      },
      "outputs": [],
      "source": [
        "#selecting the subset we want to divide further\n",
        "pol = df[df['topic'] == 'politics&biz&tech'  ]\n",
        "#retaining only those with high confidence\n",
        "print('With 0.7 confidence, retaining only', len(pol[pol.score >0.5])/len(pol),'%')\n",
        "#using the same dictionary as model10\n",
        "corpus = [dictionary_10.doc2bow(doc) for doc in pol.stems.values]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "siIAXQbUhSvk"
      },
      "outputs": [],
      "source": [
        "#saving the model to file \n",
        "df.to_csv(\"politics_biz_tech_quotes.csv.gz\", \n",
        "           index=True, \n",
        "           compression=\"gzip\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Training LDA model",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
