{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lt22f-isbleF"
      },
      "outputs": [],
      "source": [
        "from gensim.models import LdaModel\n",
        "import gensim\n",
        "from gensim.models.coherencemodel import CoherenceModel\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import spacy\n",
        "import gc\n",
        "from gensim.test.utils import datapath"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ypuV2Iegde7k"
      },
      "outputs": [],
      "source": [
        "#Ruben's stuff\n",
        "from google.cloud import bigquery\n",
        "from google.oauth2 import service_account\n",
        "import json, os\n",
        "\n",
        "# Google Cloud services\n",
        "gcp_service_account_credentials_json_filename = 'epfl-course-f41b0ed796f9.json' #need to upload the json credential files to the root directory of the google colab files\n",
        "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = gcp_service_account_credentials_json_filename\n",
        "credentials = service_account.Credentials.from_service_account_file(gcp_service_account_credentials_json_filename, scopes=['https://www.googleapis.com/auth/bigquery', 'https://www.googleapis.com/auth/drive'])\n",
        "project_id = 'epfl-course'\n",
        "bigquery_client = bigquery.Client(credentials=credentials, project=project_id)\n",
        "bigquery_client = bigquery.Client()\n",
        "\n",
        "def bq_execute_query(query, mode=\"INTERACTIVE\", wait=False, to_dataframe=False):\n",
        "    job_config = bigquery.QueryJobConfig(priority=\"bigquery.QueryPriority.{}\".format(mode)) # Run at BATCH priority, which won't count toward concurrent rate limit, otherwise INTERACTIVE.\n",
        "    query_job = bigquery_client.query(query, job_config)\n",
        "    if wait==True:\n",
        "        print(\"Executed BQ query: \", query_job.result())\n",
        "    if to_dataframe==True:\n",
        "        return(query_job.to_dataframe())\n",
        "    else:\n",
        "        return(query_job)\n",
        "\n",
        "def upload_df_to_bq(df, bq_destination_table, write_disposition=\"WRITE_APPEND\"):\n",
        "    #bq_table_name = \"epfl-course.dataset.table\"\n",
        "    job_config = bigquery.LoadJobConfig(create_disposition=\"CREATE_IF_NEEDED\", write_disposition=write_disposition) #write_disposition=\"WRITE_TRUNCATE\" in order to delete all the data from old table and insert new data\n",
        "    upload_df_to_bq_job = bigquery_client.load_table_from_dataframe(\n",
        "        df, bq_destination_table, job_config = job_config)\n",
        "    print(\"Uploaded DF to BQ: \",upload_df_to_bq_job.result()) \n",
        "\n",
        "def upload_json_to_bq(json_object, bq_table):\n",
        "    try:\n",
        "        job_config = bigquery.LoadJobConfig()\n",
        "        job_config.autodetect = False #Change to True if the table on BQ does not exits\n",
        "        job_config.max_bad_records = 0\n",
        "        job_config.ignore_unknown_values = True\n",
        "        job_config.source_format = 'NEWLINE_DELIMITED_JSON'\n",
        "        job_config.create_disposition= \"CREATE_IF_NEEDED\"\n",
        "        job_config.write_disposition= \"WRITE_APPEND\"\n",
        "        job_config.schema_to_json(schema_table)\n",
        "        job = bigquery_client.load_table_from_file(json_object, bq_table, job_config = job_config)\n",
        "        print(\"Loaded JSON to BQ table {} as job {}\".format(bq_table, job.result()))\n",
        "        assert job.job_type == 'load'\n",
        "        assert job.state == 'DONE'\n",
        "    except:\n",
        "        print(\"ERROR Could not load JSON to BQ table {} as job {}\".format(bq_table, job.result()))\n",
        "\n",
        "def upload_file_to_gcs(filename, new_filename, folder=''):\n",
        "    folder = folder if folder == '' else folder + '/'\n",
        "    bucket = storage_client.get_bucket(CLOUD_STORAGE_BUCKET)\n",
        "    blob = bucket.blob('{folder}{file}'.format(folder=folder,\n",
        "                                               file=new_filename))\n",
        "    blob.upload_from_filename(filename)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eFOEAZepdhwl"
      },
      "outputs": [],
      "source": [
        "#get the data\n",
        "sexy_query = \"\"\"\n",
        "SELECT *  \n",
        "FROM `epfl-course.ada_project.merged_table_with_annotations_and_languages_stems`\n",
        "\"\"\"\n",
        "df = bq_execute_query(sexy_query, to_dataframe=True)\n",
        "df.index = df['quoteId']\n",
        "df.head(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1-2Syvx_dBiX"
      },
      "outputs": [],
      "source": [
        "#import main model\n",
        "name_model= '1_layer_model.txt' \n",
        "name_dictionary = '1_layer_model.txt.id2word'\n",
        "main_lda = LdaModel.load(name_model)\n",
        "dictionary = gensim.corpora.Dictionary.load(name_dictionary)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AHcDxsOccA-5"
      },
      "outputs": [],
      "source": [
        "#import second model\n",
        "name_model= '1_layer_model10.txt' \n",
        "#import what's needed\n",
        "sub_lda = LdaModel.load(name_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ysp4zgRAc_tT"
      },
      "outputs": [],
      "source": [
        "#filter out on language\n",
        "sel1 = 1*(df.score > 0.80)\n",
        "sel2 = 1*df.language == 'en'\n",
        "sel3 = sel1*sel2\n",
        "print('Kept', round(100*sel3.sum()/len(df), 2), '% of points')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Predicting Main Topic"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N1U7gVDYi9MG"
      },
      "outputs": [],
      "source": [
        "#generate corpus\n",
        "corpus = [dictionary.doc2bow(doc) for doc in df.stems.values]\n",
        "main_topic = main_lda[corpus]\n",
        "\n",
        "#reformat prediction\n",
        "#transform the predictions aggregating the topics\n",
        "#this hard coded part is, unfortunately, necessary\n",
        "politics = [8, 5, 4, 0, 3]\n",
        "sport = [7]\n",
        "misc = [1, 2]\n",
        "art = [6]\n",
        "#reformat topics:\n",
        "prediction_reformatted = []\n",
        "for doc in main_topic:\n",
        "  politics_score = 0\n",
        "  misc_score = 0\n",
        "  sport_score = 0\n",
        "  art_score = 0\n",
        "  for t in doc:\n",
        "    #check if politics\n",
        "    if t[0] in politics:\n",
        "      politics_score += t[1]\n",
        "      continue\n",
        "\n",
        "    #check if misc\n",
        "    if t[0] in misc:\n",
        "      misc_score += t[1]\n",
        "      continue\n",
        "    \n",
        "    #check if sport\n",
        "    if t[0] in sport:\n",
        "      sport_score += t[1]\n",
        "      continue\n",
        "\n",
        "    #check if art\n",
        "    if t[0] in art:\n",
        "      art_score += t[1]\n",
        "      continue\n",
        "\n",
        "  erf = {\n",
        "      'politics&biz&others': politics_score,\n",
        "      'sport': sport_score,\n",
        "      'art': art_score,\n",
        "      'miscellaneous':misc_score,\n",
        "  }\n",
        "  main = dict()\n",
        "  main['main'] = max(erf, key=erf.get)\n",
        "  main['score'] = erf[main['main']]\n",
        "  erf = {\n",
        "      'scores': erf,\n",
        "      'main': main,\n",
        "      'tot': politics_score + sport_score + art_score + misc_score\n",
        "  }\n",
        "  prediction_reformatted += [erf]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5_Lnz3YTf2Gm"
      },
      "outputs": [],
      "source": [
        "#creating df out of the reformatted detected topics\n",
        "confidence = [p['main']['score'] for p in prediction_reformatted]\n",
        "topic = [p['main']['main'] for p in prediction_reformatted]\n",
        "spectrum = [p['scores'] for p in prediction_reformatted]\n",
        "df['main_topic_score'] = confidence\n",
        "df['main_topic'] = topic\n",
        "df['main_topic_spectrum'] = spectrum"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tqeAomW77nrq"
      },
      "outputs": [],
      "source": [
        "#uploading prediction of main topics\n",
        "table_id = \"epfl-course.ada_project.geo_annotated_main_topic\"\n",
        "project_id = 'epfl-course'\n",
        "bigquery_client = bigquery.Client(credentials=credentials, project=project_id)\n",
        "\n",
        "job_config = bigquery.LoadJobConfig(\n",
        "\n",
        ")\n",
        "\n",
        "client = bigquery.Client()\n",
        "job = client.load_table_from_dataframe(\n",
        "    df, table_id, job_config=job_config\n",
        ")  # Make an API request.\n",
        "job.result()  # Wait for the job to complete.\n",
        "\n",
        "table = client.get_table(table_id)  # Make an API request.\n",
        "print(\n",
        "    \"Loaded {} rows and {} columns to {}\".format(\n",
        "        table.num_rows, len(table.schema), table_id\n",
        "    )\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Predicting subtopic"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M7wikwtTkvAd"
      },
      "outputs": [],
      "source": [
        "#filtering only the politics&biz&others quotes\n",
        "pol = df[(df['main_topic'] == 'politics&biz&others') &  (df['main_topic_score'] >0.5)].copy()\n",
        "#retaining only those with high confidence\n",
        "print('With 0.5 confidence, retaining only', len(pol)/len(df[df['main_topic'] == 'politics&biz&others']),'%')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sVJTWQQtm1qF"
      },
      "outputs": [],
      "source": [
        "#generate corpus\n",
        "corpus = [dictionary.doc2bow(doc) for doc in pol.stems.values]\n",
        "sub_topic = sub_lda[corpus]\n",
        "\n",
        "#reformat prediction\n",
        "#transform the predictions aggregating the topics\n",
        "#this hard coded part is, unfortunately, necessary\n",
        "unknown = [0, 6]\n",
        "biz_tech = [1]\n",
        "econ = [2]\n",
        "war = [3]\n",
        "politics = [4]\n",
        "leg = [5]\n",
        "social = [7, 8]\n",
        "env = [9]\n",
        "#reformat topics:\n",
        "prediction_reformatted = []\n",
        "for doc in sub_topic:\n",
        "  unknown_score = 0\n",
        "  biz_tech_score = 0\n",
        "  econ_score = 0\n",
        "  war_score = 0\n",
        "  politics_score = 0\n",
        "  leg_score = 0\n",
        "  social_score = 0\n",
        "  env_score = 0\n",
        "  for t in doc:\n",
        "    if t[0] in unknown:\n",
        "      unknown_score += t[1]\n",
        "      continue\n",
        "\n",
        "    if t[0] in econ:\n",
        "      econ_score += t[1]\n",
        "      continue\n",
        "    \n",
        "    if t[0] in biz_tech:\n",
        "      biz_tech_score += t[1]\n",
        "      continue\n",
        "\n",
        "    if t[0] in politics:\n",
        "      politics_score += t[1]\n",
        "      continue\n",
        "    \n",
        "    if t[0] in leg:\n",
        "      leg_score += t[1]\n",
        "      continue\n",
        "    \n",
        "    if t[0] in social:\n",
        "      social_score += t[1]\n",
        "      continue\n",
        "\n",
        "    if t[0] in env:\n",
        "      env_score += t[1]\n",
        "      continue\n",
        "\n",
        "    if t[0] in war:\n",
        "      war_score += t[1]\n",
        "      continue\n",
        "\n",
        "  spectrum = {\n",
        "      \n",
        "      '?':unknown_score,\n",
        "      'businness and tech':biz_tech_score,\n",
        "      'economy&market':econ_score,\n",
        "      'politics':politics_score,\n",
        "      'violence&cooperation': war_score,\n",
        "      'legislation&law':leg_score,\n",
        "      'social issues':social_score,\n",
        "      'environments':env_score}\n",
        "  erf = dict()\n",
        "  erf['second_topic_spectrum'] = spectrum\n",
        "  s = sorted(spectrum, key=spectrum.get, reverse=True)\n",
        "  erf['second_topic_1'] = s[0]\n",
        "  erf['second_topic_2'] = s[1]\n",
        "  erf['second_topic_3'] = s[3]\n",
        "\n",
        "  prediction_reformatted += [erf]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PPspwhKpwPIe"
      },
      "outputs": [],
      "source": [
        "#creating df out of the reformatted detected topics\n",
        "pol['second_topic_1'] = [p['second_topic_1'] for p  in prediction_reformatted]\n",
        "pol['second_topic_2'] = [p['second_topic_2'] for p  in prediction_reformatted]\n",
        "pol['second_topic_3'] = [p['second_topic_3'] for p  in prediction_reformatted] \n",
        "pol['second_topic_spectrum'] = [p['second_topic_spectrum'] for p  in prediction_reformatted]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qiPq2-kfpwi2"
      },
      "outputs": [],
      "source": [
        "from pprint import pprint\n",
        "i = np.random.randint(0, len(pol))\n",
        "pprint(i)\n",
        "pprint(pol.iloc[i].quotation)\n",
        "print('\\n *********************')\n",
        "pprint('1:'+  pol.iloc[i]['second_topic_1'])\n",
        "pprint('2:'+ pol.iloc[i]['second_topic_2'])\n",
        "pprint('3:'+ pol.iloc[i]['second_topic_3'])\n",
        "print('\\n *********************')\n",
        "pprint(pol.iloc[i]['second_topic_spectrum'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mGIWc7Di6S8C"
      },
      "outputs": [],
      "source": [
        "df.drop(columns='quoteId', inplace = True)\n",
        "pol.drop(columns='quoteId', inplace = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5WlTvH-kzXxO"
      },
      "outputs": [],
      "source": [
        "#updload results to big query\n",
        "table_id = \"epfl-course.ada_project.geo_annotated_sub_topics\"\n",
        "project_id = 'epfl-course'\n",
        "bigquery_client = bigquery.Client(credentials=credentials, project=project_id)\n",
        "\n",
        "job_config = bigquery.LoadJobConfig(\n",
        "\n",
        ")\n",
        "\n",
        "client = bigquery.Client()\n",
        "job = client.load_table_from_dataframe(\n",
        "    pol, table_id, job_config=job_config\n",
        ")  # Make an API request.\n",
        "job.result()  # Wait for the job to complete.\n",
        "\n",
        "table = client.get_table(table_id)  # Make an API request.\n",
        "print(\n",
        "    \"Loaded {} rows and {} columns to {}\".format(\n",
        "        table.num_rows, len(table.schema), table_id\n",
        "    )\n",
        ")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "DeployingLDA.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
