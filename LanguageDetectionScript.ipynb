{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LanguageDetectionScript",
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "i6ev1cazIi9y"
      },
      "source": [
        "!pip install fasttext\n",
        "import fasttext\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import spacy\n",
        "import gc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PgHL5cktTNpK"
      },
      "source": [
        "#Ruben's stuff\n",
        "from google.cloud import bigquery\n",
        "from google.oauth2 import service_account\n",
        "import json, os\n",
        "\n",
        "# Google Cloud services\n",
        "gcp_service_account_credentials_json_filename = 'epfl-course-f41b0ed796f9.json' #need to upload the json credential files to the root directory of the google colab files\n",
        "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = gcp_service_account_credentials_json_filename\n",
        "credentials = service_account.Credentials.from_service_account_file(gcp_service_account_credentials_json_filename, scopes=['https://www.googleapis.com/auth/bigquery', 'https://www.googleapis.com/auth/drive'])\n",
        "project_id = 'epfl-course'\n",
        "bigquery_client = bigquery.Client(credentials=credentials, project=project_id)\n",
        "bigquery_client = bigquery.Client()\n",
        "\n",
        "def bq_execute_query(query, mode=\"INTERACTIVE\", wait=False, to_dataframe=False):\n",
        "    job_config = bigquery.QueryJobConfig(priority=\"bigquery.QueryPriority.{}\".format(mode)) # Run at BATCH priority, which won't count toward concurrent rate limit, otherwise INTERACTIVE.\n",
        "    query_job = bigquery_client.query(query, job_config)\n",
        "    if wait==True:\n",
        "        print(\"Executed BQ query: \", query_job.result())\n",
        "    if to_dataframe==True:\n",
        "        return(query_job.to_dataframe())\n",
        "    else:\n",
        "        return(query_job)\n",
        "\n",
        "def upload_df_to_bq(df, bq_destination_table, write_disposition=\"WRITE_APPEND\"):\n",
        "    #bq_table_name = \"epfl-course.dataset.table\"\n",
        "    job_config = bigquery.LoadJobConfig(create_disposition=\"CREATE_IF_NEEDED\", write_disposition=write_disposition) #write_disposition=\"WRITE_TRUNCATE\" in order to delete all the data from old table and insert new data\n",
        "    upload_df_to_bq_job = bigquery_client.load_table_from_dataframe(\n",
        "        df, bq_destination_table, job_config = job_config)\n",
        "    print(\"Uploaded DF to BQ: \",upload_df_to_bq_job.result()) \n",
        "\n",
        "def upload_json_to_bq(json_object, bq_table):\n",
        "    try:\n",
        "        job_config = bigquery.LoadJobConfig()\n",
        "        job_config.autodetect = False #Change to True if the table on BQ does not exits\n",
        "        job_config.max_bad_records = 0\n",
        "        job_config.ignore_unknown_values = True\n",
        "        job_config.source_format = 'NEWLINE_DELIMITED_JSON'\n",
        "        job_config.create_disposition= \"CREATE_IF_NEEDED\"\n",
        "        job_config.write_disposition= \"WRITE_APPEND\"\n",
        "        job_config.schema_to_json(schema_table)\n",
        "        job = bigquery_client.load_table_from_file(json_object, bq_table, job_config = job_config)\n",
        "        print(\"Loaded JSON to BQ table {} as job {}\".format(bq_table, job.result()))\n",
        "        assert job.job_type == 'load'\n",
        "        assert job.state == 'DONE'\n",
        "    except:\n",
        "        print(\"ERROR Could not load JSON to BQ table {} as job {}\".format(bq_table, job.result()))\n",
        "\n",
        "def upload_file_to_gcs(filename, new_filename, folder=''):\n",
        "    folder = folder if folder == '' else folder + '/'\n",
        "    bucket = storage_client.get_bucket(CLOUD_STORAGE_BUCKET)\n",
        "    blob = bucket.blob('{folder}{file}'.format(folder=folder,\n",
        "                                               file=new_filename))\n",
        "    blob.upload_from_filename(filename)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datetime import timedelta\n",
        "import datetime\n",
        "\n",
        "#generate the dates for the query\n",
        "dates = []\n",
        "#for year in range(2016, 2021):\n",
        "for year in range(2015, 2016):\n",
        "  for month in range(1,12): \n",
        "      startDate = datetime.date(year,month,1)\n",
        "      stopDate = datetime.date(year,month + 1, 1)  - timedelta(days=1)\n",
        "      startDate = startDate.strftime('%Y-%m-%d')\n",
        "      stopDate = stopDate.strftime('%Y-%m-%d')\n",
        "      dates += [\"'\"+startDate+\"' and '\"+stopDate+\"'\"]\n",
        "  #manually add december\n",
        "  startDate = datetime.date(year,12,1)\n",
        "  stopDate = datetime.date(year,12, 31)\n",
        "  startDate = startDate.strftime('%Y-%m-%d')\n",
        "  stopDate = stopDate.strftime('%Y-%m-%d')\n",
        "  dates += ['\"'+startDate+'\" and \"'+stopDate+'\"']\n",
        "\n",
        "#loading the model\n",
        "fmodel = fasttext.load_model('./lid.176.bin')\n",
        "\n",
        "#define the main query\n",
        "trunc_query = \"\"\"\n",
        "SELECT \n",
        "  quoteId,\n",
        "  quotation,\n",
        "FROM\n",
        "  `epfl-course.ada_project.quotes`\n",
        "WHERE\n",
        "  DATE(LEFT(quoteid, 10)) between \"\"\""
      ],
      "metadata": {
        "id": "tMa9PCq1oVMh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dates"
      ],
      "metadata": {
        "id": "4doy8KRRtOhx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tH3wPLEfjl9k"
      },
      "source": [
        "for dat in dates:\n",
        "  #define and execute query\n",
        "  sexy_query = trunc_query + dat + \"\\n\"\n",
        "  df = bq_execute_query(sexy_query, to_dataframe=True)\n",
        "\n",
        "  #compute the languages and score\n",
        "  temp = df.apply(lambda x: \n",
        "              fmodel.predict(x['quotation']), \n",
        "              axis = 1, \n",
        "              result_type = 'expand')\n",
        "  df = pd.concat([df, temp], axis = 'columns')\n",
        "\n",
        "  #rename columns\n",
        "  df.columns = ['quoteId', 'quotation', 'language', 'score']\n",
        "  #change the entries\n",
        "  df.score = df.score.apply(lambda x: x[0])\n",
        "  df.language = df.language.apply(lambda x: x[0][-2:])\n",
        "  #adding date\n",
        "  df['date'] = df.apply( lambda x: x['quoteId'][:10], axis = 1)\n",
        "  df['date']= pd.to_datetime(df['date'])\n",
        "\n",
        "\n",
        "  #push it on SQL\n",
        "  table_id = \"epfl-course.ada_project.lang_detect\"\n",
        "  project_id = 'epfl-course'\n",
        "  bigquery_client = bigquery.Client(credentials=credentials, project=project_id)\n",
        "\n",
        "  job_config = bigquery.LoadJobConfig(\n",
        "      schema=[\n",
        "          bigquery.SchemaField(\"quoteId\", bigquery.enums.SqlTypeNames.STRING),\n",
        "          bigquery.SchemaField(\"quotation\", bigquery.enums.SqlTypeNames.STRING),\n",
        "          bigquery.SchemaField(\"language\", bigquery.enums.SqlTypeNames.STRING),\n",
        "          bigquery.SchemaField(\"score\", bigquery.enums.SqlTypeNames.FLOAT64),\n",
        "          bigquery.SchemaField(\"date\", bigquery.enums.SqlTypeNames.DATETIME),\n",
        "      ],\n",
        "  )\n",
        "\n",
        "  job = client.load_table_from_dataframe(\n",
        "  client = bigquery.Client()\n",
        "      df, table_id, job_config=job_config\n",
        "  )  # Make an API request.\n",
        "  job.result()  # Wait for the job to complete.\n",
        "\n",
        "  table = client.get_table(table_id)  # Make an API request.\n",
        "  print(\n",
        "      \"Loaded {} rows and {} columns to {}\".format(\n",
        "          table.num_rows, len(table.schema), table_id\n",
        "      )\n",
        "  )\n",
        "  print('Done with period: ', dat)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}