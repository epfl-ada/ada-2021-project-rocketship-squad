{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Pipeline to BQ clean - all years.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TBtmFjtpAl8w",
        "outputId": "76901258-b909-49f8-e640-05e91c27e11e"
      },
      "source": [
        "!pip install tld"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tld\n",
            "  Downloading tld-0.12.6-py37-none-any.whl (412 kB)\n",
            "\u001b[?25l\r\u001b[K     |▉                               | 10 kB 15.3 MB/s eta 0:00:01\r\u001b[K     |█▋                              | 20 kB 11.6 MB/s eta 0:00:01\r\u001b[K     |██▍                             | 30 kB 9.6 MB/s eta 0:00:01\r\u001b[K     |███▏                            | 40 kB 8.8 MB/s eta 0:00:01\r\u001b[K     |████                            | 51 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |████▊                           | 61 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████▋                          | 71 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |██████▍                         | 81 kB 5.8 MB/s eta 0:00:01\r\u001b[K     |███████▏                        | 92 kB 6.0 MB/s eta 0:00:01\r\u001b[K     |████████                        | 102 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |████████▊                       | 112 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 122 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |██████████▍                     | 133 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |███████████▏                    | 143 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |████████████                    | 153 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |████████████▊                   | 163 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |█████████████▌                  | 174 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |██████████████▎                 | 184 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 194 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |████████████████                | 204 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 215 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████▌              | 225 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 235 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 245 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████▉            | 256 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████▊           | 266 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▌          | 276 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▎         | 286 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 296 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 307 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▋       | 317 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▍      | 327 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▎     | 337 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 348 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 358 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 368 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▍  | 378 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▏ | 389 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 399 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▉| 409 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 412 kB 5.0 MB/s \n",
            "\u001b[?25hInstalling collected packages: tld\n",
            "Successfully installed tld-0.12.6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aRHx1ix6A0Ta"
      },
      "source": [
        "# Loading the Quotes data in Google BigQuery (BQ)\n",
        "The goal of this file is to load the quotebank quotes datasets (one dataset available for each year) from Google drive (list of compressed JSON files) into BQ.\n",
        "\n",
        "The initial files to be loaded in BQ are in Google Drive. This python script opens by chunk the large yearly quotes files (so that they fit in memory) and then loads uploads them in BQ after some slight pre-processing.\n",
        "\n",
        "It is heavy in the first place to upload everything to BQ but the advantages that the team can enjoy later on are major. In fact, being able to operate infinitely auto-scaling queries on the whole dataset at once is crucial for insighful preliminary analysis. Since the cloud architecture is auto-scaling it is easy to run such analysis. This is why we chose the intuitive and auto-scaling platform Google BigQuery as data warehouse and no-SQL platform."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fU0ngrY5XDOq"
      },
      "source": [
        "import bz2\n",
        "import json\n",
        "import pandas as pd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k4OAYNTAFHhS"
      },
      "source": [
        "from tld import get_tld\n",
        "\n",
        "def get_domain(url):\n",
        "    res = get_tld(url, as_object=True)\n",
        "    return res.tld"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q3yHY3uIAfw-"
      },
      "source": [
        "from google.cloud import bigquery\n",
        "from google.oauth2 import service_account\n",
        "import json, os\n",
        "\n",
        "# Google Cloud services\n",
        "gcp_service_account_credentials_json_filename = '/content/epfl-course-f41b0ed796f9.json' #need to upload the json credential files to the root directory of the google colab files\n",
        "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = gcp_service_account_credentials_json_filename\n",
        "credentials = service_account.Credentials.from_service_account_file(gcp_service_account_credentials_json_filename, scopes=['https://www.googleapis.com/auth/bigquery', 'https://www.googleapis.com/auth/drive'])\n",
        "project_id = 'epfl-course'\n",
        "bigquery_client = bigquery.Client(credentials=credentials, project=project_id)\n",
        "bigquery_client = bigquery.Client()\n",
        "\n",
        "#Execute a query in BQ\n",
        "def bq_execute_query(query, mode=\"INTERACTIVE\", wait=False, to_dataframe=False):\n",
        "    job_config = bigquery.QueryJobConfig(priority=\"bigquery.QueryPriority.{}\".format(mode)) # Run at BATCH priority, which won't count toward concurrent rate limit, otherwise INTERACTIVE.\n",
        "    query_job = bigquery_client.query(query, job_config)\n",
        "    if wait==True:\n",
        "        print(\"Executed BQ query: \", query_job.result())\n",
        "    if to_dataframe==True:\n",
        "        return(query_job.to_dataframe())\n",
        "    else:\n",
        "        return(query_job)\n",
        "\n",
        "#Upload a DF to BQ\n",
        "def upload_df_to_bq_initial_upload(df, bq_destination_table, write_disposition=\"WRITE_APPEND\"):\n",
        "    #bq_table_name = \"epfl-course.dataset.table\"\n",
        "    job_config = bigquery.LoadJobConfig(create_disposition=\"CREATE_IF_NEEDED\", \n",
        "                                        write_disposition=write_disposition,\n",
        "                                        schema=[\n",
        "                                            bigquery.SchemaField(\"quids\", \"STRING\"),\n",
        "                                            bigquery.SchemaField(\"probas\", \"STRING\"),\n",
        "                                            bigquery.SchemaField(\"urls\", \"STRING\"),\n",
        "                                            bigquery.SchemaField(\"domains\", \"STRING\"),\n",
        "                                        ])\n",
        "    source_format=bigquery.SourceFormat.NEWLINE_DELIMITED_JSON\n",
        "    upload_df_to_bq_job = bigquery_client.load_table_from_dataframe(\n",
        "        df, bq_destination_table, job_config = job_config)\n",
        "    print(\"Uploaded DF to BQ: \",upload_df_to_bq_job.result()) \n",
        "\n",
        "#Upload a JSON to BQ\n",
        "def upload_json_to_bq(json_object, bq_table):\n",
        "    try:\n",
        "        job_config = bigquery.LoadJobConfig()\n",
        "        job_config.autodetect = False #Change to True if the table on BQ does not exits\n",
        "        job_config.max_bad_records = 0\n",
        "        job_config.ignore_unknown_values = True\n",
        "        job_config.source_format = 'NEWLINE_DELIMITED_JSON'\n",
        "        job_config.create_disposition= \"CREATE_IF_NEEDED\"\n",
        "        job_config.write_disposition= \"WRITE_APPEND\"\n",
        "        job = bigquery_client.load_table_from_file(json_object, bq_table, job_config = job_config)\n",
        "        print(\"Loaded JSON to BQ table {} as job {}\".format(bq_table, job.result()))\n",
        "        assert job.job_type == 'load'\n",
        "        assert job.state == 'DONE'\n",
        "    except:\n",
        "        print(\"ERROR Could not load JSON to BQ table {} as job {}\".format(bq_table, job.result()))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "uStNVkT1eag9",
        "outputId": "c87e034e-2d9c-4bb9-c35b-189a508b3873"
      },
      "source": [
        "list_of_years_to_process = range(2015, 2020) #list of the years which data has to be send to Google BigQuery(BQ)\n",
        "\n",
        "# Function uploads the JSON file to BQ and removes it from the disk to save storage\n",
        "def upload_interim_file_to_bq(filename, year):\n",
        "    print(\"uploading file: \", filename)\n",
        "    df_to_upload = pd.read_json(filename, \n",
        "                                lines=True) #open file to upload to convert it from a JSON file to panda dataframe (df)\n",
        "    bq_table = \"epfl-course.ada_project.interim_data_{}\".format(str(year))\n",
        "    upload_df_to_bq_initial_upload(df_to_upload, \n",
        "                    bq_table) #upload the df to BQ !!table name as varible\n",
        "    print(\"Uploaded df of shape: \", df_to_upload.shape)\n",
        "    os.remove(filename) #remove the JSON file not to keep them on disk storage (since we are working on Google drive, disk storage is limited)\n",
        "\n",
        "# Upload the data of one-year file to BQ\n",
        "def upload_one_year_of_data(year):\n",
        "    path_to_file = '/content/drive/MyDrive/EPFL HS21/Applied data analysis CS-401/Project/ADA team winner/Project Milestone 2/Quotebank/quotes-{}.json.bz2'.format(year)\n",
        "\n",
        "    filename_out = 'quotes-{}-domains'.format(year)\n",
        "    counter = 0\n",
        "    number_of_rows_to_upload_per_batch = 100000 #reasonable threshold for file size upload to BQ (approx. 50mb per file uploaded)\n",
        "    path_to_out_complete_previous = \"\" #a logical utility to compare the filename to the previous for-loop run\n",
        "    with bz2.open(path_to_file, 'rb') as s_file:\n",
        "        for instance in s_file: #iterates though every row of the 1-year file\n",
        "            path_to_out_complete = filename_out + str(round(counter/number_of_rows_to_upload_per_batch)).zfill(10) + \".json\" #filename of the file to be uploaded\n",
        "            with open(path_to_out_complete, 'ab') as d_file: #open in append mode 'a' with binary encoding 'b'\n",
        "                instance = json.loads(instance) # loading a sample\n",
        "                urls = instance['urls'] # extracting list of links\n",
        "                domains = []\n",
        "                for url in urls:\n",
        "                    tld = get_domain(url)\n",
        "                    domains.append(tld)\n",
        "                instance['domains'] = domains # updating the sample with domain name\n",
        "                d_file.write((json.dumps(instance)+'\\n').encode('utf-8')) # writing in the new file\n",
        "\n",
        "                #logic that allows to upload data to BQ by batches of \"number_of_rows_to_upload_per_batch\" rows\n",
        "                counter += 1\n",
        "                if path_to_out_complete_previous == path_to_out_complete: #case where file to upload is not yet complete\n",
        "                  pass\n",
        "                elif path_to_out_complete_previous == \"\": #first iteration case\n",
        "                  path_to_out_complete_previous = path_to_out_complete\n",
        "                else: # case when the file is complete with enough data to be uploaded\n",
        "                  upload_interim_file_to_bq(path_to_out_complete_previous, year)\n",
        "                  path_to_out_complete_previous = path_to_out_complete\n",
        "    upload_interim_file_to_bq(path_to_out_complete, year) #upload the last file for the year\n",
        "\n",
        "# Upload data from all years\n",
        "for year in list_of_years_to_process:\n",
        "  print(\"Processing data for year: \", year)\n",
        "  upload_one_year_of_data(year)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing data for year:  2015\n",
            "uploading file:  quotes-2015-domains0000000000.json\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/google/cloud/bigquery/_pandas_helpers.py:275: UserWarning: Unable to determine type of column 'quoteID'.\n",
            "  warnings.warn(u\"Unable to determine type of column '{}'.\".format(column))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Uploaded DF to BQ:  <google.cloud.bigquery.job.LoadJob object at 0x7f72d28731d0>\n",
            "Uploaded df of shape:  (50001, 10)\n",
            "uploading file:  quotes-2015-domains0000000001.json\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/google/cloud/bigquery/_pandas_helpers.py:275: UserWarning: Unable to determine type of column 'quoteID'.\n",
            "  warnings.warn(u\"Unable to determine type of column '{}'.\".format(column))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Uploaded DF to BQ:  <google.cloud.bigquery.job.LoadJob object at 0x7f72d27ee910>\n",
            "Uploaded df of shape:  (99999, 10)\n",
            "uploading file:  quotes-2015-domains0000000002.json\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/google/cloud/bigquery/_pandas_helpers.py:275: UserWarning: Unable to determine type of column 'quoteID'.\n",
            "  warnings.warn(u\"Unable to determine type of column '{}'.\".format(column))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Uploaded DF to BQ:  <google.cloud.bigquery.job.LoadJob object at 0x7f72d2886c50>\n",
            "Uploaded df of shape:  (100001, 10)\n",
            "uploading file:  quotes-2015-domains0000000003.json\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/google/cloud/bigquery/_pandas_helpers.py:275: UserWarning: Unable to determine type of column 'quoteID'.\n",
            "  warnings.warn(u\"Unable to determine type of column '{}'.\".format(column))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Uploaded DF to BQ:  <google.cloud.bigquery.job.LoadJob object at 0x7f72d291e6d0>\n",
            "Uploaded df of shape:  (99999, 10)\n",
            "uploading file:  quotes-2015-domains0000000004.json\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/google/cloud/bigquery/_pandas_helpers.py:275: UserWarning: Unable to determine type of column 'quoteID'.\n",
            "  warnings.warn(u\"Unable to determine type of column '{}'.\".format(column))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Uploaded DF to BQ:  <google.cloud.bigquery.job.LoadJob object at 0x7f72d2860e90>\n",
            "Uploaded df of shape:  (100001, 10)\n",
            "uploading file:  quotes-2015-domains0000000005.json\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/google/cloud/bigquery/_pandas_helpers.py:275: UserWarning: Unable to determine type of column 'quoteID'.\n",
            "  warnings.warn(u\"Unable to determine type of column '{}'.\".format(column))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Uploaded DF to BQ:  <google.cloud.bigquery.job.LoadJob object at 0x7f72d28d2f10>\n",
            "Uploaded df of shape:  (99999, 10)\n",
            "uploading file:  quotes-2015-domains0000000006.json\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/google/cloud/bigquery/_pandas_helpers.py:275: UserWarning: Unable to determine type of column 'quoteID'.\n",
            "  warnings.warn(u\"Unable to determine type of column '{}'.\".format(column))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Uploaded DF to BQ:  <google.cloud.bigquery.job.LoadJob object at 0x7f72d2929b50>\n",
            "Uploaded df of shape:  (100001, 10)\n",
            "uploading file:  quotes-2015-domains0000000007.json\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/google/cloud/bigquery/_pandas_helpers.py:275: UserWarning: Unable to determine type of column 'quoteID'.\n",
            "  warnings.warn(u\"Unable to determine type of column '{}'.\".format(column))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Uploaded DF to BQ:  <google.cloud.bigquery.job.LoadJob object at 0x7f72d2814f10>\n",
            "Uploaded df of shape:  (99999, 10)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-8772280291a7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0myear\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlist_of_years_to_process\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Processing data for year: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myear\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m   \u001b[0mupload_one_year_of_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0myear\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-6-8772280291a7>\u001b[0m in \u001b[0;36mupload_one_year_of_data\u001b[0;34m(year)\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mpath_to_out_complete_previous\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"\"\u001b[0m \u001b[0;31m#a logical utility to compare the filename to the previous for-loop run\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mbz2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_to_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0ms_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0minstance\u001b[0m \u001b[0;32min\u001b[0m \u001b[0ms_file\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m#iterates though every row of the 1-year file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m             \u001b[0mpath_to_out_complete\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfilename_out\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcounter\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mnumber_of_rows_to_upload_per_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzfill\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\".json\"\u001b[0m \u001b[0;31m#filename of the file to be uploaded\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_to_out_complete\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ab'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0md_file\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m#open in append mode 'a' with binary encoding 'b'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/bz2.py\u001b[0m in \u001b[0;36mreadline\u001b[0;34m(self, size)\u001b[0m\n\u001b[1;32m    213\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_can_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 215\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_buffer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    216\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mreadlines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/_compression.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mreadinto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mmemoryview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mview\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mview\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"B\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mbyte_view\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbyte_view\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m             \u001b[0mbyte_view\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/_compression.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, size)\u001b[0m\n\u001b[1;32m    101\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m                     \u001b[0mrawblock\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mb\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_decompressor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecompress\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrawblock\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# The previous block has been Keyboard interrupted in order to just show a couple logs. \n",
        "# The complete processing of all the files is too long to be shown in a single \n",
        "# code file notebook (here just to explain how it would work)."
      ],
      "metadata": {
        "id": "7M5Sw3JDYl0Q"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}